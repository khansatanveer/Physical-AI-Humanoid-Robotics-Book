"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[6060],{8453:(n,e,o)=>{o.d(e,{R:()=>s,x:()=>l});var t=o(6540);const a={},i=t.createContext(a);function s(n){const e=t.useContext(i);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:s(n.components),t.createElement(i.Provider,{value:e},n.children)}},9980:(n,e,o)=>{o.r(e),o.d(e,{assets:()=>r,contentTitle:()=>l,default:()=>d,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"ai-navigation-loop/project/main-project","title":"Main Project - Complete AI-Driven Navigation Loop","description":"Project Overview","source":"@site/docs/ai-navigation-loop/project/main-project.md","sourceDirName":"ai-navigation-loop/project","slug":"/ai-navigation-loop/project/main-project","permalink":"/docs/ai-navigation-loop/project/main-project","draft":false,"unlisted":false,"editUrl":"https://github.com/physical-ai-humanoid-robotics-book/physical-ai-humanoid-robotics-book/tree/main/Physical-AI-Humanoid-Robotics-book/docs/docs/ai-navigation-loop/project/main-project.md","tags":[],"version":"current","frontMatter":{"title":"Main Project - Complete AI-Driven Navigation Loop"}}');var a=o(4848),i=o(8453);const s={title:"Main Project - Complete AI-Driven Navigation Loop"},l="Main Project: Complete AI-Driven Navigation Loop",r={},c=[{value:"Project Overview",id:"project-overview",level:2},{value:"Bill of Materials (BOM)",id:"bill-of-materials-bom",level:2},{value:"Software Requirements",id:"software-requirements",level:3},{value:"Optional Hardware (for real robot testing)",id:"optional-hardware-for-real-robot-testing",level:3},{value:"System Architecture",id:"system-architecture",level:2},{value:"Step-by-Step Instructions",id:"step-by-step-instructions",level:2},{value:"Step 1: Environment Setup",id:"step-1-environment-setup",level:3},{value:"Step 2: Create the Complete Navigation Package",id:"step-2-create-the-complete-navigation-package",level:3},{value:"Step 3: Implement the Complete Navigation Node",id:"step-3-implement-the-complete-navigation-node",level:3},{value:"Step 4: Create Perception Node",id:"step-4-create-perception-node",level:3},{value:"Step 5: Create Planning Node",id:"step-5-create-planning-node",level:3},{value:"Step 6: Create Control Node",id:"step-6-create-control-node",level:3},{value:"Step 7: Create Launch File",id:"step-7-create-launch-file",level:3},{value:"Step 8: Build and Test the System",id:"step-8-build-and-test-the-system",level:3},{value:"Configuration Files",id:"configuration-files",level:2},{value:"Nav2 Configuration for Humanoid Robots",id:"nav2-configuration-for-humanoid-robots",level:3},{value:"Testing and Validation",id:"testing-and-validation",level:2},{value:"Basic Navigation Test",id:"basic-navigation-test",level:3},{value:"Complex Environment Test",id:"complex-environment-test",level:3},{value:"Performance Metrics",id:"performance-metrics",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"Debugging Tips",id:"debugging-tips",level:3},{value:"Extension Ideas",id:"extension-ideas",level:2}];function p(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"main-project-complete-ai-driven-navigation-loop",children:"Main Project: Complete AI-Driven Navigation Loop"})}),"\n",(0,a.jsx)(e.h2,{id:"project-overview",children:"Project Overview"}),"\n",(0,a.jsx)(e.p,{children:"In this project, you'll build a complete AI-driven navigation system that integrates perception, planning, and control components. This system will enable a humanoid robot to autonomously navigate through unknown environments using NVIDIA Isaac Sim for simulation and Navigation2 (Nav2) for path planning."}),"\n",(0,a.jsx)(e.h2,{id:"bill-of-materials-bom",children:"Bill of Materials (BOM)"}),"\n",(0,a.jsx)(e.h3,{id:"software-requirements",children:"Software Requirements"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Operating System"}),": Ubuntu 22.04 LTS"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"ROS 2"}),": Humble Hawksbill"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"NVIDIA Isaac Sim"}),": 2023.1.0 or later"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Isaac ROS Packages"}),": Latest compatible version"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Navigation2 (Nav2)"}),": Latest stable release"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Python"}),": 3.8 or higher"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"NVIDIA GPU"}),": RTX 3060 or better (for simulation)"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"System RAM"}),": 16GB or more"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Storage"}),": 50GB free space for Isaac Sim"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"optional-hardware-for-real-robot-testing",children:"Optional Hardware (for real robot testing)"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Humanoid Robot Platform"}),": Compatible with ROS 2"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"RGB-D Camera"}),": For perception"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"LIDAR Sensor"}),": For environment mapping"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"IMU"}),": For balance and orientation"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"system-architecture",children:"System Architecture"}),"\n",(0,a.jsx)(e.p,{children:"The complete navigation system consists of three main components:"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Perception"}),": Sensors gather environmental data (cameras, LIDAR, IMU)"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Planning"}),": Path planning algorithms determine optimal routes (Nav2)"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Control"}),": Motion controllers execute planned movements (humanoid controllers)"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"step-by-step-instructions",children:"Step-by-Step Instructions"}),"\n",(0,a.jsx)(e.h3,{id:"step-1-environment-setup",children:"Step 1: Environment Setup"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Install ROS 2 Humble"}),":"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",children:"# Setup locale\nsudo locale-gen en_US.UTF-8\nexport LANG=en_US.UTF-8\n\n# Setup sources\nsudo apt update && sudo apt install -y curl gnupg lsb-release\ncurl -sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros.key | sudo apt-key add -\nsudo sh -c 'echo \"deb [arch=$(dpkg --print-architecture)] http://packages.ros.org/ros2/ubuntu $(lsb_release -cs) main\" > /etc/apt/sources.list.d/ros2-latest.list'\n\n# Install ROS 2 packages\nsudo apt update\nsudo apt install ros-humble-desktop\nsudo apt install python3-rosdep python3-rosinstall python3-rosinstall-generator python3-wstool build-essential\n"})}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Install Navigation2"}),":"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",children:"sudo apt update\nsudo apt install ros-humble-navigation2 ros-humble-nav2-bringup\n"})}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Install Isaac ROS"}),":"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",children:"sudo apt install ros-humble-isaac-ros-* ros-humble-isaac-ros-gems\n"})}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Setup ROS workspace"}),":"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",children:"mkdir -p ~/navigation_ws/src\ncd ~/navigation_ws\ncolcon build\nsource install/setup.bash\n"})}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"step-2-create-the-complete-navigation-package",children:"Step 2: Create the Complete Navigation Package"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Create the navigation package"}),":"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",children:"cd ~/navigation_ws/src\nros2 pkg create --build-type ament_python navigation_complete_loop\ncd navigation_complete_loop\n"})}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Create the package structure"}),":"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",children:"mkdir -p navigation_complete_loop/{perception,planning,control,utils}\ntouch navigation_complete_loop/__init__.py\ntouch navigation_complete_loop/perception/__init__.py\ntouch navigation_complete_loop/planning/__init__.py\ntouch navigation_complete_loop/control/__init__.py\ntouch navigation_complete_loop/utils/__init__.py\n"})}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Update setup.py"}),":"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"import os\nfrom glob import glob\nfrom setuptools import setup\n\npackage_name = 'navigation_complete_loop'\n\nsetup(\n    name=package_name,\n    version='0.0.1',\n    packages=[package_name],\n    data_files=[\n        ('share/ament_index/resource_index/packages',\n            ['resource/' + package_name]),\n        ('share/' + package_name, ['package.xml']),\n    ],\n    install_requires=['setuptools'],\n    zip_safe=True,\n    maintainer='Your Name',\n    maintainer_email='your.email@example.com',\n    description='Complete AI-driven navigation loop example',\n    license='TODO: License declaration',\n    tests_require=['pytest'],\n    entry_points={\n        'console_scripts': [\n            'complete_navigation = navigation_complete_loop.complete_navigation:main',\n            'perception_node = navigation_complete_loop.perception_node:main',\n            'planning_node = navigation_complete_loop.planning_node:main',\n            'control_node = navigation_complete_loop.control_node:main',\n        ],\n    },\n)\n"})}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"step-3-implement-the-complete-navigation-node",children:"Step 3: Implement the Complete Navigation Node"}),"\n",(0,a.jsx)(e.p,{children:"Create the main navigation node that integrates all components:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# navigation_complete_loop/complete_navigation.py\n#!/usr/bin/env python3\n"""\nComplete AI-Driven Navigation Loop\nIntegrates perception, planning, and control systems for autonomous navigation\n"""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.action import ActionClient\nfrom rclpy.qos import QoSProfile, ReliabilityPolicy, HistoryPolicy\n\nfrom nav2_msgs.action import NavigateToPose\nfrom sensor_msgs.msg import LaserScan, Image, CameraInfo\nfrom geometry_msgs.msg import PoseStamped, PoseWithCovarianceStamped, Twist\nfrom visualization_msgs.msg import MarkerArray\nfrom std_msgs.msg import Header\nfrom builtin_interfaces.msg import Time\n\nimport cv2\nfrom cv_bridge import CvBridge\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\nimport math\nimport time\nfrom threading import Lock\n\n\nclass CompleteNavigationLoop(Node):\n    """\n    Complete navigation loop integrating perception, planning, and control\n    """\n\n    def __init__(self):\n        super().__init__(\'complete_navigation_loop\')\n\n        # QoS profile for sensor data\n        qos_profile = QoSProfile(\n            reliability=ReliabilityPolicy.BEST_EFFORT,\n            history=HistoryPolicy.KEEP_LAST,\n            depth=1\n        )\n\n        # Action client for navigation\n        self.nav_to_pose_client = ActionClient(self, NavigateToPose, \'navigate_to_pose\')\n\n        # Publishers\n        self.goal_pub = self.create_publisher(PoseStamped, \'goal_pose\', 10)\n        self.initial_pose_pub = self.create_publisher(\n            PoseWithCovarianceStamped, \'initialpose\', 10\n        )\n        self.cmd_vel_pub = self.create_publisher(Twist, \'cmd_vel\', 10)\n        self.visualization_pub = self.create_publisher(MarkerArray, \'navigation_visualization\', 10)\n\n        # Subscribers for perception\n        self.scan_sub = self.create_subscription(\n            LaserScan, \'scan\', self.scan_callback, qos_profile\n        )\n        self.image_sub = self.create_subscription(\n            Image, \'camera/image_raw\', self.image_callback, qos_profile\n        )\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo, \'camera/camera_info\', self.camera_info_callback, qos_profile\n        )\n\n        # Initialize CV bridge\n        self.cv_bridge = CvBridge()\n\n        # Navigation state\n        self.current_pose = None\n        self.current_goal = None\n        self.navigation_active = False\n        self.obstacle_detected = False\n        self.environment_map = {}\n        self.navigation_path = []\n\n        # Perception data\n        self.latest_image = None\n        self.camera_info = None\n        self.laser_data = None\n\n        # Control parameters\n        self.update_rate = 10.0  # Hz\n        self.safety_distance = 0.5  # meters\n        self.max_detection_range = 3.0  # meters\n        self.goal_tolerance = 0.25  # meters\n\n        # Threading lock for data access\n        self.data_lock = Lock()\n\n        # Timer for main navigation loop\n        self.navigation_timer = self.create_timer(1.0/self.update_rate, self.navigation_loop)\n\n        self.get_logger().info("Complete Navigation Loop initialized")\n        self.get_logger().info("Integration of perception, planning, and control systems ready")\n\n    def scan_callback(self, msg):\n        """\n        Callback for laser scan data (perception component)\n        """\n        with self.data_lock:\n            self.laser_data = msg\n            self.process_laser_data(msg)\n\n    def image_callback(self, msg):\n        """\n        Callback for camera image data (perception component)\n        """\n        with self.data_lock:\n            try:\n                self.latest_image = self.cv_bridge.imgmsg_to_cv2(msg, "bgr8")\n                self.process_image_data(self.latest_image)\n            except Exception as e:\n                self.get_logger().error(f"Error processing image: {e}")\n\n    def camera_info_callback(self, msg):\n        """\n        Callback for camera information (perception component)\n        """\n        with self.data_lock:\n            self.camera_info = msg\n\n    def process_laser_data(self, scan_msg):\n        """\n        Process laser scan data for obstacle detection (perception component)\n        """\n        obstacles = []\n        angle = scan_msg.angle_min\n\n        for range_val in scan_msg.ranges:\n            if not (math.isnan(range_val) or math.isinf(range_val)) and range_val < self.max_detection_range:\n                x = range_val * math.cos(angle)\n                y = range_val * math.sin(angle)\n                obstacles.append((x, y))\n            angle += scan_msg.angle_increment\n\n        # Update environment map with obstacle information\n        with self.data_lock:\n            self.environment_map[\'obstacles\'] = obstacles\n            self.obstacle_detected = len(obstacles) > 0\n\n    def process_image_data(self, image):\n        """\n        Process camera image for object detection (perception component)\n        """\n        # Simple color-based object detection for demonstration\n        # In a real system, this would use deep learning models\n        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n        # Define range for red color detection (for demonstration)\n        lower_red = np.array([0, 50, 50])\n        upper_red = np.array([10, 255, 255])\n        mask1 = cv2.inRange(hsv, lower_red, upper_red)\n\n        lower_red = np.array([170, 50, 50])\n        upper_red = np.array([180, 255, 255])\n        mask2 = cv2.inRange(hsv, lower_red, upper_red)\n\n        mask = mask1 + mask2\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        detected_objects = []\n        for contour in contours:\n            if cv2.contourArea(contour) > 100:  # Filter small contours\n                x, y, w, h = cv2.boundingRect(contour)\n                center_x = x + w // 2\n                center_y = y + h // 2\n                detected_objects.append((center_x, center_y, w, h))\n\n        with self.data_lock:\n            self.environment_map[\'detected_objects\'] = detected_objects\n\n    def plan_path(self, start_pose, goal_pose):\n        """\n        Plan a path from start to goal (planning component)\n        This is a simplified path planner for demonstration\n        """\n        if start_pose is None or goal_pose is None:\n            return []\n\n        # Calculate straight-line path with intermediate waypoints\n        start_pos = np.array([start_pose.position.x, start_pose.position.y])\n        goal_pos = np.array([goal_pose.position.x, goal_pose.position.y])\n\n        # Calculate direction vector\n        direction = goal_pos - start_pos\n        distance = np.linalg.norm(direction)\n\n        # Create intermediate waypoints\n        num_waypoints = max(2, int(distance / 0.5))  # 0.5m between waypoints\n        waypoints = []\n\n        for i in range(num_waypoints + 1):\n            ratio = i / num_waypoints if num_waypoints > 0 else 0\n            waypoint_pos = start_pos + direction * ratio\n\n            waypoint = PoseStamped()\n            waypoint.header.stamp = self.get_clock().now().to_msg()\n            waypoint.header.frame_id = \'map\'\n            waypoint.pose.position.x = float(waypoint_pos[0])\n            waypoint.pose.position.y = float(waypoint_pos[1])\n            waypoint.pose.position.z = 0.0\n\n            # Keep same orientation as goal for simplicity\n            waypoint.pose.orientation = goal_pose.orientation\n\n            waypoints.append(waypoint)\n\n        return waypoints\n\n    def check_path_safety(self, path):\n        """\n        Check if path is safe given current obstacle information (planning component)\n        """\n        with self.data_lock:\n            obstacles = self.environment_map.get(\'obstacles\', [])\n\n        for waypoint in path:\n            for obs_x, obs_y in obstacles:\n                dist = math.sqrt(\n                    (waypoint.pose.position.x - obs_x) ** 2 +\n                    (waypoint.pose.position.y - obs_y) ** 2\n                )\n                if dist < self.safety_distance:\n                    return False, f"Path blocked at ({waypoint.pose.position.x:.2f}, {waypoint.pose.position.y:.2f})"\n\n        return True, "Path is clear"\n\n    def send_navigation_goal(self, x, y, theta=0.0):\n        """\n        Send navigation goal to Nav2 (planning/control component)\n        """\n        if not self.nav_to_pose_client.wait_for_server(timeout_sec=5.0):\n            self.get_logger().error("Navigation action server not available")\n            return False\n\n        goal_msg = NavigateToPose.Goal()\n        goal_msg.pose.header.stamp = self.get_clock().now().to_msg()\n        goal_msg.pose.header.frame_id = \'map\'\n\n        goal_msg.pose.pose.position.x = x\n        goal_msg.pose.pose.position.y = y\n        goal_msg.pose.pose.position.z = 0.0\n\n        # Convert theta to quaternion\n        quat = R.from_euler(\'z\', theta).as_quat()\n        goal_msg.pose.pose.orientation.x = quat[0]\n        goal_msg.pose.pose.orientation.y = quat[1]\n        goal_msg.pose.pose.orientation.z = quat[2]\n        goal_msg.pose.pose.orientation.w = quat[3]\n\n        self.current_goal = goal_msg.pose.pose\n        self.navigation_active = True\n\n        self.get_logger().info(f"Sending navigation goal: ({x}, {y}, {theta})")\n\n        future = self.nav_to_pose_client.send_goal_async(\n            goal_msg,\n            feedback_callback=self.navigation_feedback_callback\n        )\n\n        future.add_done_callback(self.navigation_result_callback)\n        return True\n\n    def navigation_feedback_callback(self, feedback_msg):\n        """\n        Callback for navigation feedback (control component)\n        """\n        self.get_logger().debug("Received navigation feedback")\n        # In a real implementation, this would handle feedback from Nav2\n\n    def navigation_result_callback(self, future):\n        """\n        Callback for navigation result (control component)\n        """\n        try:\n            goal_handle = future.result()\n            if goal_handle is not None and goal_handle.status == 4:  # SUCCEEDED\n                self.get_logger().info("Navigation succeeded!")\n                self.navigation_active = False\n            else:\n                self.get_logger().info("Navigation failed or was canceled")\n                self.navigation_active = False\n        except Exception as e:\n            self.get_logger().error(f"Exception in navigation result callback: {e}")\n            self.navigation_active = False\n\n    def control_step(self):\n        """\n        Execute control actions based on current state (control component)\n        """\n        if not self.navigation_active:\n            return\n\n        # In a real system, this would send velocity commands to the robot\n        # For simulation, we\'ll just log the control actions\n        self.get_logger().debug("Executing navigation control commands")\n\n    def navigation_loop(self):\n        """\n        Main navigation loop that integrates perception, planning, and control\n        """\n        if self.current_goal is None:\n            self.get_logger().info("No goal set, waiting for navigation goal...")\n            return\n\n        # Perception: Update environment understanding\n        self.get_logger().debug("Perception: Updating environment understanding")\n\n        # Planning: Check if current path is still valid\n        self.get_logger().debug("Planning: Checking path validity")\n\n        # Check if we need to replan due to obstacles\n        with self.data_lock:\n            if self.obstacle_detected and self.current_pose:\n                self.get_logger().warn("Obstacle detected, considering replanning...")\n\n                # Create temporary goal near current position to avoid obstacle\n                temp_goal = PoseStamped()\n                temp_goal.header.frame_id = \'map\'\n                temp_goal.pose.position.x = self.current_pose.position.x + 0.5\n                temp_goal.pose.position.y = self.current_pose.position.y + 0.5\n                temp_goal.pose.orientation = self.current_pose.orientation\n\n                # Plan path to temporary goal\n                temp_path = self.plan_path(self.current_pose, temp_goal.pose)\n                is_safe, reason = self.check_path_safety(temp_path)\n\n                if not is_safe:\n                    self.get_logger().warn(f"Path to temporary goal not safe: {reason}")\n                    # Stop navigation and wait for clearer path\n                    self.navigation_active = False\n                else:\n                    self.get_logger().info("Found safe path around obstacle")\n\n        # Control: Execute navigation commands\n        self.get_logger().debug("Control: Executing navigation commands")\n        self.control_step()\n\n    def set_initial_pose(self, x, y, theta=0.0):\n        """\n        Set the initial pose of the robot\n        """\n        initial_pose = PoseWithCovarianceStamped()\n        initial_pose.header.stamp = self.get_clock().now().to_msg()\n        initial_pose.header.frame_id = \'map\'\n\n        initial_pose.pose.pose.position.x = x\n        initial_pose.pose.pose.position.y = y\n        initial_pose.pose.pose.position.z = 0.0\n\n        # Convert theta to quaternion\n        quat = R.from_euler(\'z\', theta).as_quat()\n        initial_pose.pose.pose.orientation.x = quat[0]\n        initial_pose.pose.pose.orientation.y = quat[1]\n        initial_pose.pose.pose.orientation.z = quat[2]\n        initial_pose.pose.pose.orientation.w = quat[3]\n\n        # Add small covariance values\n        initial_pose.pose.covariance[0] = 0.25  # x\n        initial_pose.pose.covariance[7] = 0.25  # y\n        initial_pose.pose.covariance[35] = 0.0685  # theta\n\n        self.initial_pose_pub.publish(initial_pose)\n        self.get_logger().info(f"Initial pose set to: ({x}, {y}, {theta})")\n\n        # Store current pose\n        self.current_pose = initial_pose.pose.pose\n\n    def execute_complete_navigation(self, start_x, start_y, goal_x, goal_y):\n        """\n        Execute complete navigation from start to goal\n        """\n        self.get_logger().info(f"Starting complete navigation from ({start_x}, {start_y}) to ({goal_x}, {goal_y})")\n\n        # Set initial pose\n        self.set_initial_pose(start_x, start_y)\n\n        # Send navigation goal\n        success = self.send_navigation_goal(goal_x, goal_y)\n\n        if success:\n            self.get_logger().info("Complete navigation loop started successfully")\n            return True\n        else:\n            self.get_logger().error("Failed to start navigation")\n            return False\n\n    def run_demonstration(self):\n        """\n        Run a demonstration of the complete navigation loop\n        """\n        self.get_logger().info("Starting complete navigation loop demonstration...")\n\n        # Example navigation: move from (0, 0) to (5, 5)\n        success = self.execute_complete_navigation(0.0, 0.0, 5.0, 5.0)\n\n        if success:\n            self.get_logger().info("Navigation demonstration started successfully")\n        else:\n            self.get_logger().error("Failed to start navigation demonstration")\n\n\ndef main(args=None):\n    """\n    Main function to run the complete navigation loop example\n    """\n    rclpy.init(args=args)\n\n    nav_loop = CompleteNavigationLoop()\n\n    try:\n        # Run the navigation demonstration\n        nav_loop.run_demonstration()\n\n        # Keep the node running\n        rclpy.spin(nav_loop)\n\n    except KeyboardInterrupt:\n        nav_loop.get_logger().info("Navigation loop interrupted by user")\n    except Exception as e:\n        nav_loop.get_logger().error(f"Error in navigation loop: {e}")\n    finally:\n        nav_loop.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,a.jsx)(e.h3,{id:"step-4-create-perception-node",children:"Step 4: Create Perception Node"}),"\n",(0,a.jsx)(e.p,{children:"Create a dedicated perception node:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# navigation_complete_loop/perception_node.py\n#!/usr/bin/env python3\n"""\nPerception Node for Navigation System\nProcesses sensor data to understand the environment\n"""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.qos import QoSProfile, ReliabilityPolicy, HistoryPolicy\n\nfrom sensor_msgs.msg import LaserScan, Image, CameraInfo, PointCloud2\nfrom geometry_msgs.msg import PointStamped\nfrom visualization_msgs.msg import MarkerArray\nfrom std_msgs.msg import Header\n\nimport cv2\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport math\nfrom scipy.spatial import distance\n\n\nclass PerceptionNode(Node):\n    """\n    Perception node for processing sensor data\n    """\n\n    def __init__(self):\n        super().__init__(\'perception_node\')\n\n        # QoS profile for sensor data\n        qos_profile = QoSProfile(\n            reliability=ReliabilityPolicy.BEST_EFFORT,\n            history=HistoryPolicy.KEEP_LAST,\n            depth=1\n        )\n\n        # Publishers\n        self.obstacle_pub = self.create_publisher(PointStamped, \'obstacles\', 10)\n        self.visualization_pub = self.create_publisher(MarkerArray, \'perception_visualization\', 10)\n\n        # Subscribers\n        self.scan_sub = self.create_subscription(\n            LaserScan, \'scan\', self.scan_callback, qos_profile\n        )\n        self.image_sub = self.create_subscription(\n            Image, \'camera/image_raw\', self.image_callback, qos_profile\n        )\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo, \'camera/camera_info\', self.camera_info_callback, qos_profile\n        )\n        self.pointcloud_sub = self.create_subscription(\n            PointCloud2, \'depth/points\', self.pointcloud_callback, qos_profile\n        )\n\n        # Initialize CV bridge\n        self.cv_bridge = CvBridge()\n\n        # Perception data\n        self.latest_image = None\n        self.camera_info = None\n        self.laser_data = None\n        self.pointcloud_data = None\n\n        # Processing parameters\n        self.update_rate = 30.0  # Hz\n\n        # Timer for perception processing\n        self.perception_timer = self.create_timer(1.0/self.update_rate, self.process_perception)\n\n        self.get_logger().info("Perception node initialized")\n\n    def scan_callback(self, msg):\n        """Handle laser scan data"""\n        self.laser_data = msg\n\n    def image_callback(self, msg):\n        """Handle camera image data"""\n        try:\n            self.latest_image = self.cv_bridge.imgmsg_to_cv2(msg, "bgr8")\n        except Exception as e:\n            self.get_logger().error(f"Error processing image: {e}")\n\n    def camera_info_callback(self, msg):\n        """Handle camera information"""\n        self.camera_info = msg\n\n    def pointcloud_callback(self, msg):\n        """Handle point cloud data"""\n        self.pointcloud_data = msg\n\n    def process_perception(self):\n        """Main perception processing loop"""\n        if self.laser_data is not None:\n            obstacles = self.process_laser_scan()\n            self.publish_obstacles(obstacles)\n\n        if self.latest_image is not None:\n            objects = self.process_camera_image()\n            self.publish_objects(objects)\n\n    def process_laser_scan(self):\n        """Process laser scan data to detect obstacles"""\n        obstacles = []\n        if self.laser_data is None:\n            return obstacles\n\n        angle = self.laser_data.angle_min\n        for range_val in self.laser_data.ranges:\n            if not (math.isnan(range_val) or math.isinf(range_val)) and range_val < 3.0:\n                x = range_val * math.cos(angle)\n                y = range_val * math.sin(angle)\n                obstacles.append((x, y))\n            angle += self.laser_data.angle_increment\n\n        return obstacles\n\n    def process_camera_image(self):\n        """Process camera image for object detection"""\n        if self.latest_image is None:\n            return []\n\n        # Simple color-based detection for demonstration\n        hsv = cv2.cvtColor(self.latest_image, cv2.COLOR_BGR2HSV)\n\n        # Detect red objects\n        lower_red = np.array([0, 50, 50])\n        upper_red = np.array([10, 255, 255])\n        mask1 = cv2.inRange(hsv, lower_red, upper_red)\n\n        lower_red = np.array([170, 50, 50])\n        upper_red = np.array([180, 255, 255])\n        mask2 = cv2.inRange(hsv, lower_red, upper_red)\n\n        mask = mask1 + mask2\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        objects = []\n        for contour in contours:\n            if cv2.contourArea(contour) > 100:  # Filter small contours\n                x, y, w, h = cv2.boundingRect(contour)\n                center_x = x + w // 2\n                center_y = y + h // 2\n\n                objects.append({\n                    \'type\': \'red_object\',\n                    \'center\': (center_x, center_y),\n                    \'bbox\': (x, y, w, h),\n                    \'area\': cv2.contourArea(contour)\n                })\n\n        return objects\n\n    def publish_obstacles(self, obstacles):\n        """Publish detected obstacles"""\n        for obs_x, obs_y in obstacles:\n            point = PointStamped()\n            point.header.stamp = self.get_clock().now().to_msg()\n            point.header.frame_id = \'map\'\n            point.point.x = obs_x\n            point.point.y = obs_y\n            point.point.z = 0.0\n            self.obstacle_pub.publish(point)\n\n    def publish_objects(self, objects):\n        """Publish detected objects"""\n        # Implementation for publishing detected objects\n        pass\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = PerceptionNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info("Perception node interrupted by user")\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,a.jsx)(e.h3,{id:"step-5-create-planning-node",children:"Step 5: Create Planning Node"}),"\n",(0,a.jsx)(e.p,{children:"Create a dedicated planning node:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# navigation_complete_loop/planning_node.py\n#!/usr/bin/env python3\n"""\nPlanning Node for Navigation System\nCreates paths from current location to goal\n"""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.action import ActionClient\n\nfrom nav2_msgs.action import NavigateToPose\nfrom geometry_msgs.msg import PoseStamped, PoseWithCovarianceStamped\nfrom sensor_msgs.msg import PointCloud2\nfrom visualization_msgs.msg import MarkerArray\n\n\nclass PlanningNode(Node):\n    """\n    Planning node for path generation\n    """\n\n    def __init__(self):\n        super().__init__(\'planning_node\')\n\n        # Action client for navigation\n        self.nav_to_pose_client = ActionClient(self, NavigateToPose, \'navigate_to_pose\')\n\n        # Publishers\n        self.path_pub = self.create_publisher(PoseStamped, \'planned_path\', 10)\n        self.visualization_pub = self.create_publisher(MarkerArray, \'planning_visualization\', 10)\n\n        # Subscribers\n        self.goal_sub = self.create_subscription(\n            PoseStamped, \'goal_pose\', self.goal_callback, 10\n        )\n        self.initial_pose_sub = self.create_subscription(\n            PoseWithCovarianceStamped, \'initialpose\', self.initial_pose_callback, 10\n        )\n        self.obstacle_sub = self.create_subscription(\n            PointCloud2, \'obstacles\', self.obstacle_callback, 10\n        )\n\n        # Planning state\n        self.current_pose = None\n        self.current_goal = None\n        self.obstacles = []\n\n        # Planning parameters\n        self.update_rate = 5.0  # Hz\n\n        # Timer for planning\n        self.planning_timer = self.create_timer(1.0/self.update_rate, self.plan_path)\n\n        self.get_logger().info("Planning node initialized")\n\n    def goal_callback(self, msg):\n        """Handle goal pose"""\n        self.current_goal = msg.pose\n\n    def initial_pose_callback(self, msg):\n        """Handle initial pose"""\n        self.current_pose = msg.pose.pose\n\n    def obstacle_callback(self, msg):\n        """Handle obstacle information"""\n        # Process obstacle data\n        self.obstacles.append(msg)\n\n    def plan_path(self):\n        """Plan path from current pose to goal"""\n        if self.current_pose is None or self.current_goal is None:\n            return\n\n        # Simple path planning implementation\n        # In a real system, this would use A*, Dijkstra, or other algorithms\n        self.get_logger().debug("Planning path from current pose to goal")\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = PlanningNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info("Planning node interrupted by user")\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,a.jsx)(e.h3,{id:"step-6-create-control-node",children:"Step 6: Create Control Node"}),"\n",(0,a.jsx)(e.p,{children:"Create a dedicated control node:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# navigation_complete_loop/control_node.py\n#!/usr/bin/env python3\n"""\nControl Node for Navigation System\nExecutes planned movements on the robot\n"""\n\nimport rclpy\nfrom rclpy.node import Node\n\nfrom geometry_msgs.msg import Twist, PoseStamped\nfrom nav_msgs.msg import Path\nfrom sensor_msgs.msg import JointState\n\n\nclass ControlNode(Node):\n    """\n    Control node for executing navigation commands\n    """\n\n    def __init__(self):\n        super().__init__(\'control_node\')\n\n        # Publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, \'cmd_vel\', 10)\n        self.joint_cmd_pub = self.create_publisher(JointState, \'joint_commands\', 10)\n\n        # Subscribers\n        self.path_sub = self.create_subscription(\n            Path, \'planned_path\', self.path_callback, 10\n        )\n        self.velocity_sub = self.create_subscription(\n            Twist, \'cmd_vel\', self.velocity_callback, 10\n        )\n\n        # Control state\n        self.current_path = []\n        self.current_velocity = Twist()\n\n        # Control parameters\n        self.control_rate = 50.0  # Hz\n\n        # Timer for control loop\n        self.control_timer = self.create_timer(1.0/self.control_rate, self.execute_control)\n\n        self.get_logger().info("Control node initialized")\n\n    def path_callback(self, msg):\n        """Handle planned path"""\n        self.current_path = msg.poses\n\n    def velocity_callback(self, msg):\n        """Handle velocity commands"""\n        self.current_velocity = msg\n\n    def execute_control(self):\n        """Execute control commands"""\n        # Send velocity commands to robot\n        if self.current_velocity.linear.x != 0 or self.current_velocity.angular.z != 0:\n            self.cmd_vel_pub.publish(self.current_velocity)\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = ControlNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info("Control node interrupted by user")\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,a.jsx)(e.h3,{id:"step-7-create-launch-file",children:"Step 7: Create Launch File"}),"\n",(0,a.jsx)(e.p,{children:"Create a launch file to start all nodes together:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:"# navigation_complete_loop/launch/navigation_launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\n\ndef generate_launch_description():\n    return LaunchDescription([\n        Node(\n            package='navigation_complete_loop',\n            executable='perception_node',\n            name='perception_node',\n            output='screen'\n        ),\n        Node(\n            package='navigation_complete_loop',\n            executable='planning_node',\n            name='planning_node',\n            output='screen'\n        ),\n        Node(\n            package='navigation_complete_loop',\n            executable='control_node',\n            name='control_node',\n            output='screen'\n        ),\n        Node(\n            package='navigation_complete_loop',\n            executable='complete_navigation',\n            name='complete_navigation_loop',\n            output='screen'\n        )\n    ])\n"})}),"\n",(0,a.jsx)(e.h3,{id:"step-8-build-and-test-the-system",children:"Step 8: Build and Test the System"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Build the package"}),":"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",children:"cd ~/navigation_ws\ncolcon build --packages-select navigation_complete_loop\nsource install/setup.bash\n"})}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Run the complete navigation system"}),":"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",children:"ros2 launch navigation_complete_loop navigation_launch.py\n"})}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Test with Isaac Sim"}),":"]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Launch Isaac Sim with a humanoid robot model"}),"\n",(0,a.jsx)(e.li,{children:"Configure the robot to publish sensor data to the expected topics"}),"\n",(0,a.jsx)(e.li,{children:"Send navigation goals using the appropriate ROS 2 interfaces"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"configuration-files",children:"Configuration Files"}),"\n",(0,a.jsx)(e.h3,{id:"nav2-configuration-for-humanoid-robots",children:"Nav2 Configuration for Humanoid Robots"}),"\n",(0,a.jsxs)(e.p,{children:["Create ",(0,a.jsx)(e.code,{children:"config/humanoid_nav2_config.yaml"}),":"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-yaml",children:'bt_navigator:\n  ros__parameters:\n    use_sim_time: True\n    global_frame: map\n    robot_base_frame: base_link\n    odom_topic: /odom\n    default_bt_xml_filename: "humanoid_navigate_w_replanning_and_recovery.xml"\n    plugin_lib_names:\n    - nav2_compute_path_to_pose_action_bt_node\n    - nav2_follow_path_action_bt_node\n    - nav2_back_up_action_bt_node\n    - nav2_spin_action_bt_node\n    - nav2_wait_action_bt_node\n\ncontroller_server:\n  ros__parameters:\n    use_sim_time: True\n    controller_frequency: 20.0\n    min_x_velocity_threshold: 0.001\n    min_y_velocity_threshold: 0.001\n    min_theta_velocity_threshold: 0.001\n    progress_checker_plugin: "progress_checker"\n    goal_checker_plugin: "goal_checker"\n    controller_plugins: ["HumanoidController"]\n\n    HumanoidController:\n      plugin: "nav2_mppi_controller::MppiController"\n      time_steps: 20\n      control_horizon: 10\n      model_dt: 0.1\n      xy_goal_tolerance: 0.25\n      yaw_goal_tolerance: 0.25\n      stateful: True\n      publish_cost_grid: False\n      speed_regulation_factor: 0.1\n      obstacle_cost_weight: 1.0\n      goal_cost_weight: 1.0\n      control_cost_weight: 0.0\n      curvature_cost_weight: 0.0\n      # Humanoid-specific parameters\n      max_humanoid_vel_x: 0.3\n      min_humanoid_vel_x: -0.1\n      max_humanoid_vel_theta: 0.5\n      max_humanoid_acc_x: 0.5\n      max_humanoid_acc_theta: 0.5\n      step_size_limit: 0.3\n      foot_separation: 0.2\n      stance_time: 0.1\n\nlocal_costmap:\n  local_costmap:\n    ros__parameters:\n      update_frequency: 5.0\n      publish_frequency: 2.0\n      global_frame: odom\n      robot_base_frame: base_link\n      use_sim_time: True\n      rolling_window: true\n      width: 6\n      height: 6\n      resolution: 0.05\n      robot_radius: 0.3\n'})}),"\n",(0,a.jsx)(e.h2,{id:"testing-and-validation",children:"Testing and Validation"}),"\n",(0,a.jsx)(e.h3,{id:"basic-navigation-test",children:"Basic Navigation Test"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Launch the navigation system"}),"\n",(0,a.jsx)(e.li,{children:"Set initial pose at (0, 0)"}),"\n",(0,a.jsx)(e.li,{children:"Set goal at (5, 5)"}),"\n",(0,a.jsx)(e.li,{children:"Verify the robot navigates to the goal"}),"\n",(0,a.jsx)(e.li,{children:"Check that obstacles are detected and avoided"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"complex-environment-test",children:"Complex Environment Test"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Create a complex environment with multiple obstacles"}),"\n",(0,a.jsx)(e.li,{children:"Test navigation in this environment"}),"\n",(0,a.jsx)(e.li,{children:"Verify path replanning when obstacles are detected"}),"\n",(0,a.jsx)(e.li,{children:"Check that the robot can recover from navigation failures"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"performance-metrics",children:"Performance Metrics"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Navigation success rate"}),"\n",(0,a.jsx)(e.li,{children:"Average time to reach goal"}),"\n",(0,a.jsx)(e.li,{children:"Path efficiency (actual path length vs. straight-line distance)"}),"\n",(0,a.jsx)(e.li,{children:"Obstacle detection accuracy"}),"\n",(0,a.jsx)(e.li,{children:"System response time"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,a.jsx)(e.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"No path found"}),": Check that the goal is reachable and not in an obstacle"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Robot not moving"}),": Verify that velocity commands are being published"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Oscillating behavior"}),": Adjust controller parameters for stability"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Perception errors"}),": Check sensor calibration and data quality"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"debugging-tips",children:"Debugging Tips"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Use RViz2 to visualize the navigation system state"}),"\n",(0,a.jsx)(e.li,{children:"Monitor ROS 2 topics to verify data flow"}),"\n",(0,a.jsx)(e.li,{children:"Check TF transforms for coordinate frame issues"}),"\n",(0,a.jsxs)(e.li,{children:["Use ROS 2 tools like ",(0,a.jsx)(e.code,{children:"ros2 bag"})," to record and replay data"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"extension-ideas",children:"Extension Ideas"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Learning-based navigation"}),": Implement reinforcement learning for navigation"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Semantic mapping"}),": Add object recognition and semantic mapping"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Multi-robot coordination"}),": Extend to multiple robots"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Dynamic obstacle prediction"}),": Predict and avoid moving obstacles"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Human-aware navigation"}),": Consider human presence in navigation planning"]}),"\n"]})]})}function d(n={}){const{wrapper:e}={...(0,i.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(p,{...n})}):p(n)}}}]);