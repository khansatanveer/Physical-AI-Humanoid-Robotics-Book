"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[2933],{8453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>l});var i=s(6540);const a={},t=i.createContext(a);function r(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),i.createElement(t.Provider,{value:n},e.children)}},8569:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>m,frontMatter:()=>r,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"module-3-digital-twin/assets/sample-sensor-data","title":"Sample Sensor Data Outputs for Digital Twin","description":"Overview","source":"@site/docs/module-3-digital-twin/assets/sample-sensor-data.md","sourceDirName":"module-3-digital-twin/assets","slug":"/module-3-digital-twin/assets/sample-sensor-data","permalink":"/docs/module-3-digital-twin/assets/sample-sensor-data","draft":false,"unlisted":false,"editUrl":"https://github.com/physical-ai-humanoid-robotics-book/physical-ai-humanoid-robotics-book/tree/main/Physical-AI-Humanoid-Robotics-book/docs/docs/module-3-digital-twin/assets/sample-sensor-data.md","tags":[],"version":"current","frontMatter":{}}');var a=s(4848),t=s(8453);const r={},l="Sample Sensor Data Outputs for Digital Twin",o={},d=[{value:"Overview",id:"overview",level:2},{value:"LiDAR Sensor Sample Data",id:"lidar-sensor-sample-data",level:2},{value:"ROS Message Format",id:"ros-message-format",level:3},{value:"Sample LiDAR Output",id:"sample-lidar-output",level:3},{value:"Analysis of Sample Data",id:"analysis-of-sample-data",level:3},{value:"Depth Camera Sample Data",id:"depth-camera-sample-data",level:2},{value:"ROS Message Format",id:"ros-message-format-1",level:3},{value:"Sample Depth Image Characteristics",id:"sample-depth-image-characteristics",level:3},{value:"Sample Depth Data Interpretation",id:"sample-depth-data-interpretation",level:3},{value:"Analysis of Sample Data",id:"analysis-of-sample-data-1",level:3},{value:"IMU Sensor Sample Data",id:"imu-sensor-sample-data",level:2},{value:"ROS Message Format",id:"ros-message-format-2",level:3},{value:"Sample IMU Output",id:"sample-imu-output",level:3},{value:"Analysis of Sample Data",id:"analysis-of-sample-data-2",level:3},{value:"Fused Sensor State Sample Data",id:"fused-sensor-state-sample-data",level:2},{value:"Custom Message Format",id:"custom-message-format",level:3},{value:"Sample Fused State Output",id:"sample-fused-state-output",level:3},{value:"Analysis of Sample Data",id:"analysis-of-sample-data-3",level:3},{value:"Unity Visualization Data Format",id:"unity-visualization-data-format",level:2},{value:"Coordinate System Conversion",id:"coordinate-system-conversion",level:3},{value:"Conversion Functions",id:"conversion-functions",level:3},{value:"Performance Metrics",id:"performance-metrics",level:2},{value:"LiDAR Performance",id:"lidar-performance",level:3},{value:"Depth Camera Performance",id:"depth-camera-performance",level:3},{value:"IMU Performance",id:"imu-performance",level:3},{value:"System Performance",id:"system-performance",level:3},{value:"Validation Criteria",id:"validation-criteria",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"sample-sensor-data-outputs-for-digital-twin",children:"Sample Sensor Data Outputs for Digital Twin"})}),"\n",(0,a.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(n.p,{children:"This document provides sample outputs from the various sensors implemented in the digital twin system. These examples demonstrate the format and characteristics of realistic sensor data that can be expected from the simulation."}),"\n",(0,a.jsx)(n.h2,{id:"lidar-sensor-sample-data",children:"LiDAR Sensor Sample Data"}),"\n",(0,a.jsx)(n.h3,{id:"ros-message-format",children:"ROS Message Format"}),"\n",(0,a.jsxs)(n.p,{children:["The LiDAR sensor publishes ",(0,a.jsx)(n.code,{children:"sensor_msgs/LaserScan"})," messages with the following structure:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"std_msgs/Header header\n  uint32 seq\n  time stamp\n  string frame_id\nfloat32 angle_min\nfloat32 angle_max\nfloat32 angle_increment\nfloat32 time_increment\nfloat32 scan_time\nfloat32 range_min\nfloat32 range_max\nfloat32[] ranges\nfloat32[] intensities\n"})}),"\n",(0,a.jsx)(n.h3,{id:"sample-lidar-output",children:"Sample LiDAR Output"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:'header:\n  seq: 12345\n  stamp:\n    sec: 1234\n    nanosec: 567890123\n  frame_id: "lidar_link"\nangle_min: -3.1415927\nangle_max: 3.1415927\nangle_increment: 0.017453292\ntime_increment: 0.0\nscan_time: 0.0\nrange_min: 0.1\nrange_max: 30.0\nranges:\n- 2.45\n- 2.46\n- 2.47\n- 2.48\n- 2.49\n- 2.50\n- 2.51\n- 2.52\n- 2.53\n- 2.54\n- 2.55\n- 2.56\n- 2.57\n- 2.58\n- 2.59\n- 2.60\n- 2.61\n- 2.62\n- 2.63\n- 2.64\n- 2.65\n- 2.66\n- 2.67\n- 2.68\n- 2.69\n- 2.70\n- 2.71\n- 2.72\n- 2.73\n- 2.74\n- 2.75\n- 2.76\n- 2.77\n- 2.78\n- 2.79\n- 2.80\n- 2.81\n- 2.82\n- 2.83\n- 2.84\n- 2.85\n- 2.86\n- 2.87\n- 2.88\n- 2.89\n- 2.90\n- 2.91\n- 2.92\n- 2.93\n- 2.94\n- 2.95\n- 2.96\n- 2.97\n- 2.98\n- 2.99\n- 3.00\nintensities: []\n'})}),"\n",(0,a.jsx)(n.h3,{id:"analysis-of-sample-data",children:"Analysis of Sample Data"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"The sample shows a simple scenario with gradually increasing distances"}),"\n",(0,a.jsx)(n.li,{children:"All range values are within the valid range (0.1m to 30.0m)"}),"\n",(0,a.jsx)(n.li,{children:"The sensor is detecting objects at distances between 2.45m and 3.00m"}),"\n",(0,a.jsx)(n.li,{children:"This pattern might represent an open corridor or empty space with distant walls"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"depth-camera-sample-data",children:"Depth Camera Sample Data"}),"\n",(0,a.jsx)(n.h3,{id:"ros-message-format-1",children:"ROS Message Format"}),"\n",(0,a.jsxs)(n.p,{children:["The depth camera publishes ",(0,a.jsx)(n.code,{children:"sensor_msgs/Image"})," messages with 32-bit float encoding:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"std_msgs/Header header\n  uint32 seq\n  time stamp\n  string frame_id\nuint32 height\nuint32 width\nstring encoding\nuint8 is_bigendian\nuint32 step\nuint8[] data\n"})}),"\n",(0,a.jsx)(n.h3,{id:"sample-depth-image-characteristics",children:"Sample Depth Image Characteristics"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Resolution"}),": 640x480 pixels"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Encoding"}),": 32FC1 (32-bit float, 1 channel)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Data Type"}),": Each pixel represents distance in meters"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Range"}),": 0.1m to 10.0m (valid depth range)"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"sample-depth-data-interpretation",children:"Sample Depth Data Interpretation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"# Sample depth values from a 640x480 image (center region)\n# Values in meters\n\n2.50  2.49  2.48  2.47  2.46  2.45  2.44  2.43\n2.51  2.50  2.49  2.48  2.47  2.46  2.45  2.44\n2.52  2.51  2.50  2.49  2.48  2.47  2.46  2.45\n2.53  2.52  2.51  2.50  2.49  2.48  2.47  2.46\n2.54  2.53  2.52  2.51  2.50  2.49  2.48  2.47\n2.55  2.54  2.53  2.52  2.51  2.50  2.49  2.48\n2.56  2.55  2.54  2.53  2.52  2.51  2.50  2.49\n2.57  2.56  2.55  2.54  2.53  2.52  2.51  2.50\n"})}),"\n",(0,a.jsx)(n.h3,{id:"analysis-of-sample-data-1",children:"Analysis of Sample Data"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"The depth values represent a scene with objects at approximately 2.5m distance"}),"\n",(0,a.jsx)(n.li,{children:"Small variations represent realistic sensor noise"}),"\n",(0,a.jsx)(n.li,{children:"This could represent a wall or obstacle at medium range"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"imu-sensor-sample-data",children:"IMU Sensor Sample Data"}),"\n",(0,a.jsx)(n.h3,{id:"ros-message-format-2",children:"ROS Message Format"}),"\n",(0,a.jsxs)(n.p,{children:["The IMU sensor publishes ",(0,a.jsx)(n.code,{children:"sensor_msgs/Imu"})," messages:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"std_msgs/Header header\ngeometry_msgs/Quaternion orientation\ngeometry_msgs/Vector3 orientation_covariance\ngeometry_msgs/Vector3 angular_velocity\ngeometry_msgs/Vector3 angular_velocity_covariance\ngeometry_msgs/Vector3 linear_acceleration\ngeometry_msgs/Vector3 linear_acceleration_covariance\n"})}),"\n",(0,a.jsx)(n.h3,{id:"sample-imu-output",children:"Sample IMU Output"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:'header:\n  seq: 54321\n  stamp:\n    sec: 1234\n    nanosec: 987654321\n  frame_id: "imu_link"\norientation:\n  x: 0.012\n  y: 0.008\n  z: 0.003\n  w: 0.999\norientation_covariance: [0.01, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.01]\nangular_velocity:\n  x: 0.001\n  y: -0.002\n  z: 0.003\nangular_velocity_covariance: [0.01, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.01]\nlinear_acceleration:\n  x: 0.1\n  y: -0.05\n  z: 9.7\nlinear_acceleration_covariance: [0.01, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.01]\n'})}),"\n",(0,a.jsx)(n.h3,{id:"analysis-of-sample-data-2",children:"Analysis of Sample Data"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Orientation"}),": Close to identity quaternion (mostly upright position)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Angular Velocity"}),": Very small values indicating minimal rotation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Linear Acceleration"}),": ~9.7 m/s\xb2 in Z direction (gravity) with small X/Y components"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Covariance"}),": Small values indicating high confidence in measurements"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"fused-sensor-state-sample-data",children:"Fused Sensor State Sample Data"}),"\n",(0,a.jsx)(n.h3,{id:"custom-message-format",children:"Custom Message Format"}),"\n",(0,a.jsxs)(n.p,{children:["The sensor fusion node publishes ",(0,a.jsx)(n.code,{children:"std_msgs/Float32MultiArray"})," messages containing fused state:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"std_msgs/MultiArrayLayout layout\nstd_msgs/MultiArrayType data_type\nfloat32[] data\n"})}),"\n",(0,a.jsx)(n.h3,{id:"sample-fused-state-output",children:"Sample Fused State Output"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:'layout:\n  dim:\n  - label: "state_vector"\n    size: 10\n    stride: 10\n  data_offset: 0\ndata_type: 8  # FLOAT32\ndata:\n- 1.25    # x position\n- 0.87    # y position\n- 0.02    # z position\n- 0.015   # qx orientation\n- 0.008   # qy orientation\n- 0.002   # qz orientation\n- 0.999   # qw orientation\n- 0.12    # x velocity\n- 0.08    # y velocity\n- 0.01    # z velocity\n'})}),"\n",(0,a.jsx)(n.h3,{id:"analysis-of-sample-data-3",children:"Analysis of Sample Data"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Position"}),": Robot at (1.25, 0.87, 0.02) in world coordinates"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Orientation"}),": Slight tilt from identity (quaternion representation)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Velocity"}),": Slow movement in positive x and y directions"]}),"\n",(0,a.jsx)(n.li,{children:"This represents a robot moving slowly while maintaining upright orientation"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"unity-visualization-data-format",children:"Unity Visualization Data Format"}),"\n",(0,a.jsx)(n.h3,{id:"coordinate-system-conversion",children:"Coordinate System Conversion"}),"\n",(0,a.jsx)(n.p,{children:"When transmitting sensor data to Unity, coordinate systems must be converted:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"ROS/Gazebo"}),": X forward, Y left, Z up"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Unity"}),": X right, Y up, Z forward"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"conversion-functions",children:"Conversion Functions"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-csharp",children:"// Convert ROS coordinates to Unity coordinates\npublic Vector3 RosToUnity(Vector3 rosVector)\n{\n    return new Vector3(rosVector.z, rosVector.x, rosVector.y);\n}\n\n// Convert ROS quaternion to Unity quaternion\npublic Quaternion RosToUnity(Quaternion rosQuaternion)\n{\n    return new Quaternion(rosQuaternion.z, rosQuaternion.x, rosQuaternion.y, rosQuaternion.w);\n}\n"})}),"\n",(0,a.jsx)(n.h2,{id:"performance-metrics",children:"Performance Metrics"}),"\n",(0,a.jsx)(n.h3,{id:"lidar-performance",children:"LiDAR Performance"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Update Rate"}),": 10 Hz (100ms interval)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Data Points"}),": 360 ranges per scan"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Message Size"}),": ~1.5 KB per message"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Processing Time"}),": < 5ms per scan"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"depth-camera-performance",children:"Depth Camera Performance"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Update Rate"}),": 30 Hz (33ms interval)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Resolution"}),": 640x480 = 307,200 pixels"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Message Size"}),": ~1.2 MB per image (32-bit float)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Processing Time"}),": < 20ms per frame"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"imu-performance",children:"IMU Performance"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Update Rate"}),": 100 Hz (10ms interval)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Message Size"}),": ~100 bytes per message"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Processing Time"}),": < 1ms per message"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"system-performance",children:"System Performance"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Combined Sensor Data Rate"}),": ~1.2 MB/s at peak"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"CPU Usage"}),": < 15% for sensor processing"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Memory Usage"}),": ~50 MB for data buffering"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Latency"}),": < 50ms end-to-end for sensor fusion"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"validation-criteria",children:"Validation Criteria"}),"\n",(0,a.jsx)(n.p,{children:"The sample data above meets the following validation criteria:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Range Validity"}),": All sensor values are within expected operational ranges"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Message Frequency"}),": Sensors publish at configured rates"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Data Consistency"}),": Values are physically plausible"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Noise Characteristics"}),": Realistic noise patterns present"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Coordinate Alignment"}),": Proper frame_id conventions followed"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Performance"}),": System maintains real-time operation"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"These samples provide a reference for expected sensor behavior in the digital twin system and can be used for validating custom implementations."})]})}function m(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}}}]);