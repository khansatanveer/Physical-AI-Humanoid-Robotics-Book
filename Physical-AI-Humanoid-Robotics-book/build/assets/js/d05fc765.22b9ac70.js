"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[9959],{2135:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>u,frontMatter:()=>s,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"vla/introduction","title":"Introduction to Vision-Language-Action Models","description":"This is a placeholder file for the VLA models introduction chapter.","source":"@site/docs/vla/introduction.md","sourceDirName":"vla","slug":"/vla/introduction","permalink":"/docs/vla/introduction","draft":false,"unlisted":false,"editUrl":"https://github.com/physical-ai-humanoid-robotics-book/physical-ai-humanoid-robotics-book/tree/main/Physical-AI-Humanoid-Robotics-book/docs/docs/vla/introduction.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Introduction to Vision-Language-Action Models"},"sidebar":"tutorialSidebar","previous":{"title":"Gazebo-Unity Integration Workflow","permalink":"/docs/module-3-digital-twin/workflow-sketch"},"next":{"title":"VLA Models Integration","permalink":"/docs/vla/integration"}}');var t=i(4848),r=i(8453);const s={sidebar_position:1,title:"Introduction to Vision-Language-Action Models"},l="Introduction to Vision-Language-Action Models",a={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Content Coming Soon",id:"content-coming-soon",level:2},{value:"Try It Yourself",id:"try-it-yourself",level:2},{value:"Quiz",id:"quiz",level:2},{value:"Further Reading",id:"further-reading",level:2}];function d(e){const n={h1:"h1",h2:"h2",header:"header",li:"li",p:"p",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"introduction-to-vision-language-action-models",children:"Introduction to Vision-Language-Action Models"})}),"\n",(0,t.jsx)(n.p,{children:"This is a placeholder file for the VLA models introduction chapter."}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Define Vision-Language-Action (VLA) models"}),"\n",(0,t.jsx)(n.li,{children:"Understand the role of VLA models in robotics"}),"\n",(0,t.jsx)(n.li,{children:"Identify key VLA model architectures"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"content-coming-soon",children:"Content Coming Soon"}),"\n",(0,t.jsx)(n.p,{children:"This section will cover Vision-Language-Action models, including:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Overview of VLA models and their importance in robotics"}),"\n",(0,t.jsx)(n.li,{children:"The integration of perception, reasoning, and action"}),"\n",(0,t.jsx)(n.li,{children:"RT-2 (Robotics Transformer 2) and other VLA architectures"}),"\n",(0,t.jsx)(n.li,{children:"How VLA models enable robots to understand natural language commands"}),"\n",(0,t.jsx)(n.li,{children:"Applications to humanoid robotics tasks"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"try-it-yourself",children:"Try It Yourself"}),"\n",(0,t.jsx)(n.p,{children:"[Placeholder for hands-on exercises and examples]"}),"\n",(0,t.jsx)(n.h2,{id:"quiz",children:"Quiz"}),"\n",(0,t.jsx)(n.p,{children:"[Placeholder for chapter quiz with 3-5 questions]"}),"\n",(0,t.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,t.jsx)(n.p,{children:"[Placeholder for additional resources and references]"})]})}function u(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>l});var o=i(6540);const t={},r=o.createContext(t);function s(e){const n=o.useContext(r);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),o.createElement(r.Provider,{value:n},e.children)}}}]);